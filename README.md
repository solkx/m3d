## [M3D: A Multimodal, Multilingual and Multitask Dataset for Grounded Document-level Information Extraction](https://arxiv.org/abs/2412.04026)

### 1. Dataset Introduction
M3D is a multimodal multilingual multitask dataset, which has the following features: 
* (1) It contains paired document-level text and video to enrich multimodal information; 
* (2) It supports two widely-used languages, namely English and Chinese; 
* (3) It includes more multimodal IE tasks such as entity recognition, entity chain extraction, relation extraction and visual grounding. 
* (4) In addition, our dataset introduces an unexplored theme, i.e., biography, enriching the domains of multimodal IE resources.

### 2. Dataset
Our dataset can be found in the `data` and Image download [link](https://drive.google.com/file/d/1BCEh_8dE8hGcAtsScZmijPIXpIDPmtzu/view?usp=drive_link)

### 3. Training and Test
Our benchmark model is trained and tested through `python main.py`

## 4. Cite (Important !!!)
```
@article{liu2024m,
  title={M3D: A Multimodal, Multilingual and Multitask Dataset for Grounded Document-level Information Extraction},
  author={Liu, Jiang and Li, Bobo and Yang, Xinran and Yang, Na and Fei, Hao and Zhang, Mingyao and Li, Fei and Ji, Donghong},
  journal={arXiv preprint arXiv:2412.04026},
  year={2024}
}